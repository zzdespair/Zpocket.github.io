{"pages":[{"title":"","text":"个人简介 分享很喜欢的老罗的一段话： “每一个生命来到世间都注定改变世界，别无选择。要么变得好一点，要么变得坏一点。你如果走进社会为了生存为了什么不要脸的理由，变成了一个恶心的成年人社会中的一员，那你就把这个世界变得恶心了一点点。如果你一生刚正不阿，如果你一生耿直，没有做任何恶心的事情，没做对别人有害的事情，一辈子拼了老命勉强把自己身边的几个人照顾好了，没有成名没有发财，没有成就伟大的事业，然后耿着脖子一生正直，到了七八十岁耿着脖子去世了。你这一生是不是没有改变世界？你还是改变世界了，你把这个世界变得美好了一点点。因为世界上又多了一个好人。“ 善恶终有报,天道好轮回。不信抬头看,苍天饶过谁。无论何时何地，我们都要保持一颗积极乐观、善良感恩的心。但行好事莫问前程，永远年轻，永远热内盈眶，永远保持正能量。💪💪💪💪💪💪冲鸭！！！！ -&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;个人信息：计算机科学与技术专业从事JAVA后端开发码畜一枚坚信代码改变世界 博客信息 网站采用的Icarus主题 追求尽可能的简洁，清晰，易用。 在Icarus主题之上进行了部分修改。 更新日志：–2020.01.18：icarus3.0适配–2019.11.17：增加深色主题开关–2019.10.30：去图，精简卡片–2019.10.22：改版部分显示，优化速度–2019.10.16：文章列表加上评论数显示–2019.10.13：改版评论–2019.09.25：图片、资源接入CDN免费jsDelivr、文章加入置顶–2019.09.19：开源博客代码–2019.09.19：修改布局，拉伸布局，更宽的展示–2019.09.18：修改友链ui为一行三个，并适配移动端，暗黑模式文章增加评论链接，增加留言链接–2019.09.14：增加精简next主题–2019.09.14：利用中秋节放假，重做了首页的热门推荐、加个widget最新评论框、归档页加入文章贡献概览面板 本站推荐索引 博客主题相关 github Issue 作为博客微型数据库的应用 github page网站cdn优化加速 博客源码分享 博客换肤的一种实现方式思路 博客中gitalk最新评论的获取 博客图片上传picgo工具github图传使用 安装、部分配置icarus主题中文版 技术知识点 Java并发知识点 法律法规 法律法规数据库 中华人民共和国国旗法 中华人民共和国宪法 中华人民共和国消费者权益保护法 中华人民共和国刑事诉讼法 中华人民共和国婚姻法 中华人名共和国网络安全法 中华人民共和国劳动法 其他 网易云音乐歌单分享 计划2020计划 2019.12.31 2020-GOALS 跑两三场马拉松 2019计划 2018.12.31/21:59:00-&gt;更新于2019.12.31 2019-GOALS 购买的专业书籍至少看完一遍（并发、重构、设计模式…）-&gt; 95% 额外： 追了很多剧 总结： 有优点有缺点，没坚持下来的还是太多，追了太多剧。以后多学习，多思考！ 时间轴记录","link":"/about/index.html"},{"title":"","text":"唐艺昕 李沁 李一桐 gakki 图片搜集于互联网，侵权请留言，马上处理😊。","link":"/album/index.html"},{"title":"","text":"&nbsp;&nbsp;听听音乐 音乐播放器由mePlayer提供，布局参照网友博客所作，感谢作者的辛勤付出。更多音乐分享请查看歌单。 &nbsp;&nbsp;看看视频 ->点击以下条目开始播放视频,向下滑动查看更多","link":"/media/index.html"},{"title":"","text":"申请友链须知 原则上只和技术类博客交换，但不包括含有和色情、暴力、政治敏感的网站。 不和剽窃、侵权、无诚信的网站交换，优先和具有原创作品的网站交换。 申请请提供：站点名称、站点链接、站点描述、logo或头像（不要设置防盗链）。 排名不分先后，刷新后重排，更新信息后请留言告知。 会定期清理很久很久不更新的、不符合要求的友链，不再另行通知。 本站不存储友链图片，如果友链图片换了无法更新。图片裂了的会替换成默认图，需要更换的请留言告知。 本站友链信息如下，申请友链前请先添加本站信息： 网站图标：https://removeif.github.io/images/avatar.jpg 网站名称：辣椒の酱 网站地址：https://removeif.github.io 网站简介：后端开发，技术分享 加载中，稍等几秒...","link":"/friend/index.html"},{"title":"","text":"来而不往非礼也畅所欲言，有留必应","link":"/message/index.html"},{"title":"音乐歌单收藏","text":"--- 温馨提示：选择喜欢的音乐双击播放，由于版权原因部分不能播放。如果喜欢歌单收藏一下，去网易云都能播放哟！","link":"/music/index.html"},{"title":"","text":"碎碎念 tips：github登录后按时间正序查看、可点赞加❤️、本插件地址..「+99次查看」 碎碎念加载中，请稍等... $.getScript(\"/js/gitalk_self.min.js\", function () { var gitalk = new Gitalk({ clientID: '46a9f3481b46ea0129d8', clientSecret: '79c7c9cb847e141757d7864453bcbf89f0655b24', id: '666666', repo: 'issue_database', owner: 'removeif', admin: \"removeif\", createIssueManually: true, distractionFreeMode: false }); gitalk.render('comment-container1'); });","link":"/self-talking/index.html"}],"posts":[{"title":"Elasticsearch 笔记","text":"摘要 因最近项目需求，开始使用Elasticsearch。 因此遇到不少坑，和不习惯的地方，本篇文章不定期更细，主要用作笔记和记录 Elasticsearch 安装此处省略 Elasticsearch 基本使用启动 Elasticsearch输入一下指令开启服务 1sudo systemctl restart elasticsearch.service 本地环境是Mac下搭建的Homestead环境，内核信息如下 1Linux version 4.15.0-96-generic (buildd@lgw01-amd64-004) (gcc version 7.5.0 (Ubuntu 7.5.0-3ubuntu1~18.04)) #97-Ubuntu SMP Wed Apr 1 03:25:46 UTC 2020 初次使用创建最简单的数据 12345curl -X PUT \"localhost:9200/customer/_doc/1?pretty\" -H 'Content-Type: application/json' -d'{ \"name\": \"John Doe\"}' Es所有Api访问方式都是Http访问，所以使用curl方式连接和传输数据 如果该请求customer尚不存在，此请求将自动创建该索引，添加ID为的新文档1，并存储该name字段并为其建立索引。 由于这是一个新文档，因此响应显示该操作的结果是创建了该文档的版本1： 返回体： 1234567891011121314{ &quot;_index&quot; : &quot;customer&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_version&quot; : 1, &quot;result&quot; : &quot;created&quot;, &quot;_shards&quot; : { &quot;total&quot; : 2, &quot;successful&quot; : 2, &quot;failed&quot; : 0 }, &quot;_seq_no&quot; : 26, &quot;_primary_term&quot; : 4} 新文档可立即从群集中的任何节点使用。您可以使用指定文档ID的GET请求检索它： 1curl -X GET \"localhost:9200/customer/_doc/1?pretty\" 该响应表明找到了具有指定ID的文档，并显示了已索引的原始源字段。 123456789101112{ \"_index\" : \"customer\", \"_type\" : \"_doc\", \"_id\" : \"1\", \"_version\" : 1, \"_seq_no\" : 26, \"_primary_term\" : 4, \"found\" : true, \"_source\" : { \"name\": \"John Doe\" }} 批量添加文件如果您有很多要添加的文档，则可以使用批量API批量提交。使用批量处理批处理文档操作比单独提交请求要快得多，因为它可以最大程度地减少网络往返次数。 最佳批处理大小取决于许多因素：文档大小和复杂性，索引编制和搜索负载以及群集可用的资源。一个好的起点是批处理1,000至5,000个文档，总有效负载在5MB至15MB之间。从那里，您可以尝试找到最佳位置。 要将一些数据导入Elasticsearch，您可以开始搜索和分析： 下载accounts.json样本数据集。此随机生成的数据集中的文档代表具有以下信息的用户帐户： 12345678910111213{ \"account_number\": 0, \"balance\": 16623, \"firstname\": \"Bradshaw\", \"lastname\": \"Mckenzie\", \"age\": 29, \"gender\": \"F\", \"address\": \"244 Columbus Place\", \"employer\": \"Euron\", \"email\": \"bradshawmckenzie@euron.com\", \"city\": \"Hobucken\", \"state\": \"CO\"} bank使用以下_bulk请求将帐户数据添加到索引中： 123curl -H \"Content-Type: application/json\" -XPOST \"localhost:9200/bank/_bulk?pretty&amp;refresh\" --data-binary \"@accounts.json\" curl \"localhost:9200/_cat/indices?v\" 响应表明成功添加了1,000个文档。 12health status index uuid pri rep docs.count docs.deleted store.size pri.store.sizeyellow open bank l7sSYV2cQXmu6_4rJWVIww 5 1 1000 0 128.6kb 128.6kb 开始搜索将一些数据摄取到Elasticsearch索引后，您可以通过将请求发送到_search端点来对其进行搜索。要访问全套搜索功能，请使用Elasticsearch Query DSL在请求正文中指定搜索条件。您可以在请求URI中指定要搜索的索引的名称。 例如，以下请求检索bank 按帐号排序的索引中的所有文档： 12345678curl -X GET \"localhost:9200/bank/_search?pretty\" -H 'Content-Type: application/json' -d'{ \"query\": { \"match_all\": {} }, \"sort\": [ { \"account_number\": \"asc\" } ]}' 默认情况下，hits响应部分包括与搜索条件匹配的前10个文档： 123456789101112131415161718192021222324252627282930313233{ \"took\" : 63, \"timed_out\" : false, \"_shards\" : { \"total\" : 5, \"successful\" : 5, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\": 1000, \"relation\": \"eq\" }, \"max_score\" : null, \"hits\" : [ { \"_index\" : \"bank\", \"_type\" : \"_doc\", \"_id\" : \"0\", \"sort\": [0], \"_score\" : null, \"_source\" : {\"account_number\":0,\"balance\":16623,\"firstname\":\"Bradshaw\",\"lastname\":\"Mckenzie\",\"age\":29,\"gender\":\"F\",\"address\":\"244 Columbus Place\",\"employer\":\"Euron\",\"email\":\"bradshawmckenzie@euron.com\",\"city\":\"Hobucken\",\"state\":\"CO\"} }, { \"_index\" : \"bank\", \"_type\" : \"_doc\", \"_id\" : \"1\", \"sort\": [1], \"_score\" : null, \"_source\" : {\"account_number\":1,\"balance\":39225,\"firstname\":\"Amber\",\"lastname\":\"Duke\",\"age\":32,\"gender\":\"M\",\"address\":\"880 Holmes Lane\",\"employer\":\"Pyrami\",\"email\":\"amberduke@pyrami.com\",\"city\":\"Brogan\",\"state\":\"IL\"} }, ... ] }} 该响应还提供有关搜索请求的以下信息： took – Elasticsearch运行查询所需的时间（以毫秒为单位） timed_out –搜索请求是否超时 _shards –搜索了多少个分片，以及成功，失败或跳过了多少个分片。 max_score –找到的最相关文件的分数 hits.total.value -找到了多少个匹配的文档 hits.sort -文档的排序位置（不按相关性得分排序时） hits._score-文档的相关性得分（使用时不适用match_all） 每个搜索请求都是独立的：Elasticsearch不会在请求中维护任何状态信息。要翻阅搜索结果，请在您的请求中指定from和size参数。 例如，以下请求的匹配数为10到19： 12345678910curl -X GET \"localhost:9200/bank/_search?pretty\" -H 'Content-Type: application/json' -d'{ \"query\": { \"match_all\": {} }, \"sort\": [ { \"account_number\": \"asc\" } ], \"from\": 10, \"size\": 10}' 现在，您已经了解了如何提交基本的搜索请求，可以开始构建比有趣的查询match_all。 要在字段中搜索特定术语，可以使用match查询。例如，以下请求搜索该address字段以查找地址包含mill或的客户lane： 12345curl -X GET \"localhost:9200/bank/_search?pretty\" -H 'Content-Type: application/json' -d'{ \"query\": { \"match\": { \"address\": \"mill lane\" } }}' 要执行词组搜索而不是匹配单个词，请使用 match_phrase代替match。例如，以下请求仅匹配包含短语的地址mill lane： 12345curl -X GET \"localhost:9200/bank/_search?pretty\" -H 'Content-Type: application/json' -d'{ \"query\": { \"match_phrase\": { \"address\": \"mill lane\" } }}' 要构造更复杂的查询，可以使用bool查询来组合多个查询条件。您可以根据需要（必须匹配），期望（应匹配）或不期望（必须不匹配）指定条件。 例如，以下请求在bank索引中搜索属于40岁客户的帐户，但不包括居住在爱达荷州（ID）的任何人： 1234567891011121314curl -X GET \"localhost:9200/bank/_search?pretty\" -H 'Content-Type: application/json' -d'{ \"query\": { \"bool\": { \"must\": [ { \"match\": { \"age\": \"40\" } } ], \"must_not\": [ { \"match\": { \"state\": \"ID\" } } ] } }}' 布尔查询中的每个must，should和must_not元素称为查询子句。文档满足每个条款must或 should条款的标准的程度有助于文档的相关性得分。分数越高，文档就越符合您的搜索条件。默认情况下，Elasticsearch返回按这些相关性分数排名的文档。 must_not子句中的条件被视为过滤器。它影响文件是否包含在结果中，但不会影响文件的评分方式。您还可以显式指定任意过滤器，以基于结构化数据包括或排除文档。 例如，以下请求使用范围过滤器将结果限制为余额在20,000美元到30,000美元（含）之间的帐户。 1234567891011121314151617curl -X GET \"localhost:9200/bank/_search?pretty\" -H 'Content-Type: application/json' -d'{ \"query\": { \"bool\": { \"must\": { \"match_all\": {} }, \"filter\": { \"range\": { \"balance\": { \"gte\": 20000, \"lte\": 30000 } } } } }}' 使用聚合Elasticsearch聚合使您能够获取有关搜索结果的元信息，并回答诸如“德克萨斯州有多少个帐户持有人？”之类的问题。或“田纳西州的平均帐户余额是多少？” 您可以在一个请求中搜索文档，过滤命中并使用汇总分析结果。 例如，以下请求使用terms汇总将bank索引中的所有帐户按状态分组，并按降序返回帐户数量最多的十个州： 123456789101112curl -X GET \"localhost:9200/bank/_search?pretty\" -H 'Content-Type: application/json' -d'{ \"size\": 0, \"aggs\": { \"group_by_state\": { \"terms\": { \"field\": \"state.keyword\" } } }}' buckets响应中的是的值state字段中。该 doc_count节目在每个州帐户数量。例如，您可以看到ID（爱达荷州）有27个帐户。因为请求set size=0，所以响应仅包含聚合结果。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455{ \"took\": 29, \"timed_out\": false, \"_shards\": { \"total\": 5, \"successful\": 5, \"skipped\" : 0, \"failed\": 0 }, \"hits\" : { \"total\" : { \"value\": 1000, \"relation\": \"eq\" }, \"max_score\" : null, \"hits\" : [ ] }, \"aggregations\" : { \"group_by_state\" : { \"doc_count_error_upper_bound\": 20, \"sum_other_doc_count\": 770, \"buckets\" : [ { \"key\" : \"ID\", \"doc_count\" : 27 }, { \"key\" : \"TX\", \"doc_count\" : 27 }, { \"key\" : \"AL\", \"doc_count\" : 25 }, { \"key\" : \"MD\", \"doc_count\" : 25 }, { \"key\" : \"TN\", \"doc_count\" : 23 }, { \"key\" : \"MA\", \"doc_count\" : 21 }, { \"key\" : \"NC\", \"doc_count\" : 21 }, { \"key\" : \"ND\", \"doc_count\" : 21 }, { \"key\" : \"ME\", \"doc_count\" : 20 }, { \"key\" : \"MO\", \"doc_count\" : 20 } ] } }} 您可以组合聚合以构建更复杂的数据汇总。例如，以下请求将一个avg聚合嵌套在先前的 group_by_state聚合中，以计算每个状态的平均帐户余额。 12345678910111213141516171819curl -X GET \"localhost:9200/bank/_search?pretty\" -H 'Content-Type: application/json' -d'{ \"size\": 0, \"aggs\": { \"group_by_state\": { \"terms\": { \"field\": \"state.keyword\" }, \"aggs\": { \"average_balance\": { \"avg\": { \"field\": \"balance\" } } } } }}' 您可以通过指定terms聚合内的顺序来使用嵌套聚合的结果进行排序，而不是按计数对结果进行排序： 12345678910111213141516171819202122curl -X GET \"localhost:9200/bank/_search?pretty\" -H 'Content-Type: application/json' -d'{ \"size\": 0, \"aggs\": { \"group_by_state\": { \"terms\": { \"field\": \"state.keyword\", \"order\": { \"average_balance\": \"desc\" } }, \"aggs\": { \"average_balance\": { \"avg\": { \"field\": \"balance\" } } } } }}' 除了这些基本的存储桶和指标聚合外，Elasticsearch还提供了专门的聚合，用于在多个字段上操作并分析特定类型的数据，例如日期，IP地址和地理数据。您还可以将单个聚合的结果馈送到管道聚合中，以进行进一步分析。 聚合提供的核心分析功能可启用高级功能，例如使用机器学习来检测异常。 Elasticsearch 搜索URL搜索通过查询参数提供查询。URI搜索往往更简单，最适合测试。 1curl -X GET \"localhost:9200/my-index-000001/_search?q=user.id:kimchy&amp;pretty\" 转换为DSL搜索就是 123456789curl -X GET \"localhost:9200/my-index-000001,my-index-000002/_search?pretty\" -H 'Content-Type: application/json' -d'{ \"query\": { \"match\": { \"user.id\": \"kimchy\" } }}' Request body searches (DSL搜索)通过API请求的JSON主体提供查询。这些查询是用查询DSL编写的。在日常项目开发中请求正文搜索是重中之重。 _source 过滤字段默认情况下，搜索响应中的每个_source匹配都包含document ，这是对文档建立索引时提供的整个JSON对象。如果在搜索响应中仅需要某些源字段，则可以使用源过滤来限制返回源的哪些部分。 仅使用文档源返回字段有一些限制： 该_source字段不包含多字段或 字段别名。同样，源中的字段也不包含使用copy_to映射参数复制的值。 由于_source在lucene中将_s存储为单个字段，因此即使仅需要少量字段，也必须加载和解析整个源对象。 为避免这些限制，您可以： 使用docvalue_fields 参数获取选定字段的值。当返回相当少量的支持doc值的字段（例如关键字和日期）时，这是一个不错的选择。 使用stored_fields参数获取特定存储字段的值。（使用store映射选项的字段。） 使用场景： 12345678910111213curl -X GET \"localhost:9200/_search?pretty\" -H 'Content-Type: application/json' -d'{ \"_source\": { \"includes\": [ \"obj1.*\", \"obj2.*\" ], \"excludes\": [ \"*.description\" ] }, \"query\": { \"term\": { \"user.id\": \"kimchy\" } }}' 名词 解释 _source 源过滤 若传值为false，不返回全部资源 includes 则仅返回与其模式之一匹配的源字段 excludes 从子集中排除字段 aggs 聚合聚合功能为ES注入了统计分析的血统，使用户在面对大数据提取统计指标时变得游刃有余。聚合允许我们向数据提出一些複杂的问题，虽然他的功能完全不同于搜索，但他们其实使用了相同的数据结构，这表示聚合的执行速度很快，并且就像搜索一样几乎是实时的。 聚合的两个主要的概念，分别是 桶 和 指标 桶(Buckets) : 满足特定条件的文档的集合 当聚合开始被执行，每个文档会决定符合哪个桶的条件，如果匹配到，文档将放入相应的桶并接着进行聚合操作 像是一个员工属于男性桶或者女性桶，日期2014-10-28属于十月桶，也属于2014年桶 桶可以被嵌套在其他桶里面 像是北京能放在中国桶裡，而中国桶能放在亚洲桶裡 Elasticsearch提供了很多种类型的桶，像是时间、最受欢迎的词、年龄区间、地理位置桶等等，不过他们在根本上都是通过同样的原理进行操作，也就是基于条件来划分文档，一个文档只要符合条件，就可以加入那个桶，因此一个文档可以同时加入很多桶 指标(Metrics) : 对桶内的文档进行统计计算 桶能让我们划分文档到有意义的集合， 但是最终我们需要的是对这些桶内的文档进行一些指标的计算 指标通常是简单的数学运算(像是min、max、avg、sum），而这些是通过当前桶中的文档的值来计算的，利用指标能让你计算像平均薪资、最高出售价格、95%的查询延迟这样的数据 aggs 聚合的模板 当query和aggs一起存在时，会先执行query的主查询，主查询query执行完后会搜出一批结果，而这些结果才会被拿去aggs拿去做聚合 另外要注意aggs后面会先接一层自定义的这个聚合的名字，然后才是接上要使用的聚合桶 如果有些情况不在意查询结果是什麽，而只在意aggs的结果，可以把size设为0，如此可以让返回的hits结果集是0，加快返回的速度 一个aggs裡可以有很多个聚合，每个聚合彼此间都是独立的，因此可以一个聚合拿来统计数量、一个聚合拿来分析数据、一个聚合拿来计算标准差…，让一次搜索就可以把想要做的事情一次做完 像是此例就定义了3个聚合，分别是custom_name1、custom_name2、custom_name3 aggs可以嵌套在其他的aggs裡面，而嵌套的桶能作用的文档集范围，是外层的桶所输出的结果集 1234567891011121314151617181920212223GET 127.0.0.1/mytest/doc/_search{ \"query\": { ... }, \"size\": 0, \"aggs\": { \"custom_name1\": { // aggs后面接著的是一个自定义的name \"桶\": { ... } // 再来才是接桶 }, \"custom_name2\": { // 一个aggs裡可以有很多聚合 \"桶\": { ... } }, \"custom_name3\": { \"桶\": { ..... }, \"aggs\": { // aggs可以嵌套在别的aggs裡面 \"in_name\": { // 记得使用aggs需要先自定义一个name \"桶\": { ... } // in_name的桶作用的文档是custom_name3的桶的结果 } } } }} 返回体 123456789101112131415161718192021{ \"hits\": { \"total\": 8, \"max_score\": 0, \"hits\": [] //因为size设为0，所以没有查询结果返回 }, \"aggregations\": { \"custom_name1\": { ... }, \"custom_name2\": { ... }, \"custom_name3\": { ... , \"in_name\": { .... } } } } 完整例子： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576GET /metric.*/_search // 指定index查询{ \"size\":0, // 返回数限定0 \"aggs\": { // 聚合开启关键字，固定格式 \"2\": { // 对这个聚合起名字 \"date_histogram\": { // 这个聚合的类型 ，这里是时间直方图聚合 \"fixed_interval\": \"1d\", // 该聚合类型内的关键字，在各个类型内有详解 \"field\": \"datetime\", \"min_doc_count\": 1, \"time_zone\": \"Asia/Shanghai\" }, \"aggs\": { ---- \"avg_grade\": { | \"avg\": { | \"field\": \"value\" | } | }, | \"max_price\": { | \"max\": { |--&gt; // 针对上面聚合后产生的文档，再次进行聚合操作 \"field\": \"value\" | } | }, | \"min_price\": { | \"min\": { | \"field\": \"value\" | } | } | } ---- } //\"3\": { } --- //\"4\": { } | --&gt; // 对文档进行其他聚合 //\"5\": { } --- }, \"_source\": { // 过滤设置 \"includes\": [ \"value\", \"datetime\" ], \"excludes\": [] }, \"query\": { // 查询 \"bool\": { \"filter\": { \"range\": { \"datetime\": { \"time_zone\": \"Asia/Shanghai\", \"format\":\"yyyy-MM-dd HH:mm:ss\", \"gte\": \"2020-07-28 19:46:08\", \"lte\": \"2020-07-31 20:46:08\" } } }, \"must\": [ { \"match\": { \"sname\": \"gwaf_monitor_srv\" } }, { \"match\": { \"mname\": \"server_monitor\" } }, { \"match\": { \"iname\": \"mem\" } } ] } }, \"sort\": [ // 排序 { \"datetime\": \"asc\" } ]} 返回： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566{ \"took\" : 6, \"timed_out\" : false, \"_shards\" : { \"total\" : 24, \"successful\" : 24, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 3012, \"relation\" : \"eq\" }, \"max_score\" : null, \"hits\" : [ ] }, \"aggregations\" : { \"2\" : { \"buckets\" : [ { \"key_as_string\" : \"2020-07-29 00:00:00\", \"key\" : 1595952000000, \"doc_count\" : 1175, \"max_price\" : { \"value\" : 292276.0 }, \"min_price\" : { \"value\" : 5144.0 }, \"avg_grade\" : { \"value\" : 13365.889361702128 } }, { \"key_as_string\" : \"2020-07-30 00:00:00\", \"key\" : 1596038400000, \"doc_count\" : 1432, \"max_price\" : { \"value\" : 188092.0 }, \"min_price\" : { \"value\" : 5436.0 }, \"avg_grade\" : { \"value\" : 10735.731843575419 } }, { \"key_as_string\" : \"2020-07-31 00:00:00\", \"key\" : 1596124800000, \"doc_count\" : 405, \"max_price\" : { \"value\" : 23408.0 }, \"min_price\" : { \"value\" : 5648.0 }, \"avg_grade\" : { \"value\" : 9392.266666666666 } } ] } }} 常用桶terms 桶针对某个field的值进行分组，field有几种值就分成几组 terms桶在进行分组时，会爲此field中的每种值创建一个新的桶 要注意此 “terms桶” 和平常用在主查询query中的 “查找terms” 是不同的东西 具体实例，先插入数据 123{ \"color\": \"red\" }{ \"color\": \"green\" }{ \"color\": [\"red\", \"blue\"] } 执行搜索 1234567891011121314GET 127.0.0.1/mytest/doc/_search{ \"query\": { \"match_all\": {} }, \"size\": 0, \"aggs\": { \"my_name\": { \"terms\": { \"field\": \"color\" // 使用color来进行分组 } } }} terms聚合应该是字段类型keyword或适用于存储桶聚合的任何其他数据类型。为了与它一起使用，text您将需要启用 fielddata 结果 1234567891011121314151617181920212223{ ... \"aggregations\": { \"my_name\": { \"doc_count_error_upper_bound\": 0, \"sum_other_doc_count\": 0, \"buckets\": [ { \"key\": \"blue\", \"doc_count\": 1 }, { \"key\": \"red\", \"doc_count\": 2 //表示color为red的文档有2个，此例中就是 {\"color\": \"red\"} 和 {\"color\": [\"red\", \"blue\"]}这两个文档 }, { \"key\": \"green\", \"doc_count\": 1 } ] } }} 因为color总共有3种值，red、blue、green，所以terms桶为他们产生了3个bucket，并计算了每个bucket中符合的文档有哪些 bucket和bucket间是独立的，也就是说一个文档可以同时符合好几个bucket，像是{&quot;color&quot;: [&quot;red&quot;, &quot;blue&quot;]}就同时符合了red和blue bucket 参数 解释 例子 field 分组依据 “field”: “color” order 可根据 _count, _key 使用 asc,desc 或者聚合名词 例1 “order”: { “_count”: “asc” } min_doc_count 最小文件数 “min_doc_count”: 10 missing 将缺少参数赋予值 “missing”: “N/A” order例子 1： 12345678910111213141516171819202122232425262728GET /_search{ \"aggs\": { \"genres\": { \"terms\": { \"field\": \"genre\", \"order\": { \"max_play_count\": \"desc\" } }, \"aggs\": { \"max_play_count\": { \"max\": { \"field\": \"play_count\" } } } } }}GET /_search{ \"aggs\": { \"genres\": { \"terms\": { \"field\": \"genre\", \"order\": { \"playback_stats.max\": \"desc\" } }, \"aggs\": { \"playback_stats\": { \"stats\": { \"field\": \"play_count\" } } } } }} order其他的高阶用方法参考官方文档ES官方文档-aggs-order date_histogram 日期直方图例如，以下是一个汇总，要求在日历时间中每个月的存储分区间隔： 1234567891011121314151617181920POST /sales/_search?size=0{ \"aggs\": { \"sales_over_time\": { \"date_histogram\": { \"field\": \"date\", // 数据中字段名 \"calendar_interval\": \"month\" // 间隔时间 //\"fixed_interval\": \"30d\" 固定间隔时间 与calendar_interval同级，不可同时使用 \"format\": \"yyyy-MM-dd\" // 指定时间格式 \"time_zone\": \"Asia/Shanghai\" // 时区调整 \"offset\": \"+6h\" // 从上午六点开始分组 \"min_doc_count\": 0, // 为空的话则填充0 \"extended_bounds\": { // 需要填充0的范围 \"min\": 1533556800000, \"max\": 1533806520000 } } } }} calendar_interval 参数 解释 minute (1m) 分钟 hour (1h) 小时 day (1d) 天 week (1w) 周 month (1M) 月 quarter (1q) 季 year (1y) 年 fixed_interval 是固定间隔时间 与日历感知间隔相比，固定间隔是固定数量的SI单位，并且永远不会偏离，无论它们位于日历上的哪个位置。一秒始终由1000毫秒组成。这允许以任意多个受支持单位指定固定间隔。 但是，这意味着固定的时间间隔不能表示其他单位，例如月份，因为一个月的持续时间不是固定的数量。尝试指定日历间隔（例如月份或季度）将引发异常。 fixed_interval 参数 解释 milliseconds (ms) 毫秒 seconds (s) 定义为1000毫秒 hours (h) 所有时间都是从00分00秒开始。定义为每个60分钟（3,600,000毫秒） days (d) 所有时间都是从最早的时间开始，通常是00:00:00（午夜）。定义为24小时（86,400,000毫秒） histogram 直方图执行汇总时，将评估每个文档的价格字段并将其四舍五入到最接近的存储桶-例如，如果价格为32且存储桶大小为，5则四舍五入将产生结果30，因此文档将“落入”与密钥关联的桶30。为了使它更加正式，这里使用了舍入函数： 1bucket_key = Math.floor((value - offset) / interval) * interval + offset interval必须是正的小数，而offset必须在小数[0, interval) （小数大于或等于0且小于interval） 以下代码段根据产品的“ price间隔”对产品进行“存储桶” 50： 123456789101112131415POST /sales/_search?size=0{ \"aggs\": { \"prices\": { \"histogram\": { \"field\": \"price\", \"interval\": 50, \"extended_bounds\": { //限定范围 \"min\": 0, \"max\": 500 } } } }} 返回如下： 1234567891011121314151617181920212223242526272829{ ... \"aggregations\": { \"prices\": { \"buckets\": [ { \"key\": 0.0, \"doc_count\": 1 }, { \"key\": 50.0, \"doc_count\": 1 }, { \"key\": 100.0, \"doc_count\": 0 }, { \"key\": 150.0, \"doc_count\": 2 }, { \"key\": 200.0, \"doc_count\": 3 } ] } }} 常用指标聚合avg 平均值123456POST /exams/_search?size=0{ \"aggs\": { \"avg_grade\": { \"avg\": { \"field\": \"grade\" } } }} 12345678{ ... \"aggregations\": { \"avg_grade\": { \"value\": 75.0 } }} max 最大值123456POST /sales/_search?size=0{ \"aggs\": { \"max_price\": { \"max\": { \"field\": \"price\" } } }} 12345678{ ... \"aggregations\": { \"max_price\": { \"value\": 200.0 } }} min 最小值123456POST /sales/_search?size=0{ \"aggs\": { \"min_price\": { \"min\": { \"field\": \"price\" } } }} 12345678{ ... \"aggregations\": { \"min_price\": { \"value\": 10.0 } }} sum 总和12345678910111213POST /sales/_search?size=0{ \"query\": { \"constant_score\": { \"filter\": { \"match\": { \"type\": \"hat\" } } } }, \"aggs\": { \"hat_prices\": { \"sum\": { \"field\": \"price\" } } }} 12345678{ ... \"aggregations\": { \"hat_prices\": { \"value\": 450.0 } }} median_absolute_deviation 中位数12345678910111213141516GET reviews/_search{ \"size\": 0, \"aggs\": { \"review_average\": { \"avg\": { \"field\": \"rating\" } }, \"review_variability\": { \"median_absolute_deviation\": { \"field\": \"rating\" } } }} 1234567891011{ ... \"aggregations\": { \"review_average\": { \"value\": 3.0 }, \"review_variability\": { \"value\": 2.0 } }} Query DSL搜索es的Query DSL以_search为endpoint，主要分为叶子查询语句（Leaf Query) 和 复合查询语句 (Compound query clauses)。 叶子查询语句（Leaf Query) 用于查询某个特定的字段，如 match , term 或 range 等 其中主要包括两类：单词匹配和全文匹配: 单词匹配(Term Level Query)不对查询语句进行分词处理，直接匹配该字段的倒排索引，包括term、terms、range查询语句: term queryterm查询语句不会对查询语句进行分词处理，直接拿查询输入的文本去检索，如下是官方文档测试案例，非常清晰: 12345678GET /_search{ \"query\": { \"term\": { \"exact_value\": \"Quick Foxes!\" //如果exact_value字段类型是keyword，则有结果，若为text则无结果 } }} terms query返回在提供的字段中包含一个或多个确切术语的文档。该terms查询与term查询相同，除了可以搜索多个值。 12345678GET /_search{ \"query\": { \"terms\": { \"user.id\": [ \"kimchy\", \"elkbee\" ] } }} 同时支持文档源的查询 参数 解释 index （必需，字符串）从中获取字段值的索引名称。 id （必需，字符串）要从中获取字段值的文档的ID。 path （必需，字符串）要从中获取字段值的字段名称。Elasticsearch将这些值用作查询的搜索词。如果字段值包含嵌套的内部对象的数组，则可以使用点表示法语法访问这些对象。 routing （可选，字符串）从中获取术语值的文档的自定义路由值。如果在为文档建立索引时提供了自定义路由值，则此参数是必需的。 示例： 123456789101112GET my-index-000001/_search?pretty{ \"query\": { \"terms\": { \"color\" : { \"index\" : \"my-index-000001\", \"id\" : \"2\", \"path\" : \"color\" } } }} range query范围查询主要用于date或number类型的字段查询中，和term、terms查询一样，不进行查询时分词: 123456789101112GET /_search{ \"query\": { \"range\": { \"age\": { \"gte\": 10, \"lte\": 20, \"boost\": 2.0 } } }} 参数 解释 gt （可选）大于 gte （可选）大于或等于 lt （可选）小于 lte （可选）小于或等于 format （可选，字符串）用于转换date查询中的值的日期格式。有关有效语法，请参见format。 time_zone （可选，字符串） 用于将查询中的值转换为UTC的协调世界时（UTC）偏移量或 IANA时区date。 boost （可选，float）浮点数，用于降低或增加查询的 相关性分数。默认为1.0 在range范围查询中，es提供了一种更加简便的日期计算，now表示当前时间，时间单位y:年，M:月，d:天，H:时，m:分，s:秒，所以now-3y就表示当前时间减去3年后的时间。 123456789101112GET /_search{ \"query\": { \"range\": { \"timestamp\": { \"time_zone\": \"+01:00\", \"gte\": \"2020-01-01T00:00:00\", \"lte\": \"now\" } } }} prefix query 词项前缀查询12345GET /_search{ \"query\": { \"prefix\" : { \"user\" : \"ki\" } }} wildcard query 通配符查询123456789101112131415161718GET /_search{ \"query\": { \"wildcard\" : { \"user\" : \"ki*y\" } }}GET /_search{ \"query\": { \"wildcard\": { \"user\": { \"value\": \"ki*y\", \"boost\": 2 } } } } regexp query 正则查询1234567891011121314151617181920GET /_search{ \"query\": { \"regexp\":{ \"name.first\": \"s.*y\" } }}GET /_search{ \"query\": { \"regexp\":{ \"name.first\":{ \"value\":\"s.*y\", \"boost\":1.2 } } }} 正则语法参考：https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-regexp-query.html#regexp-syntax fuzzy query 模糊查询123456789101112131415161718192021GET /_search{ \"query\": { \"fuzzy\" : { \"user\" : \"ki\" } }}GET /_search{ \"query\": { \"fuzzy\" : { \"user\" : { \"value\": \"ki\", \"boost\": 1.0, \"fuzziness\": 2, \"prefix_length\": 0, \"max_expansions\": 100 } } }} ids 根据文档id查询123456789GET /_search{ \"query\": { \"ids\" : { \"type\" : \"_doc\", \"values\" : [\"1\", \"4\", \"100\"] } }} 全文匹配(Full Text Query)对指定的text类型的字段进行全文检索，会先对查询语句进行分词处理，例如你输入的查询文本是”我在马路边”，es在分词器的作用下就会分词为”我”、”在”、”马路”这么几个单词，然后再去匹配。 match querymatch query是最基本的基于全文检索的字段类查询语法： 12345678910GET /_search{ \"query\": { \"match\": { \"message\": { \"query\": \"this is a test\" } } }} 那么这里经过分词后的this、is、a、test各个单词之间就是默认为”或”的匹配关系，可以通过operator关键字来显式设置各个单词间的匹配关系，如下: 1234567891011GET /_search{ \"query\": { \"match\": { \"message\": { \"query\": \"this is a test\", \"operator\": \"and\" } } }} 也可以通过minimum_should_match参数来设置控制需要匹配的单词数，比如你的文档里username存的内容是”this is a java enginer”，那么通过下面的查询语句可以控制查询文本最少要匹配到”java”、”enginer”: 1234567891011GET /_search{ \"query\": { \"match\": { \"message\": { \"query\": \"java enginer\", \"minimum_should_match\": 2 } } }} 还可用max_expansions 指定模糊匹配的最大词项数，默认是50。比如：反向索引中有 100 个词项与 ucen 模糊匹配，只选用前50 个。 12345678910111213GET ftq/_search{ \"query\": { \"match\": { \"content\": { \"query\": \"ucen elatic java\", \"fuzziness\": 2, \"minimum_should_match\": 2, \"max_expansions \": 50 } } }} match_phrase querymatch_phrase也是对字段进行检索，和match的区别在于：match_query是有顺序要求的，而match是无序的。 12345678GET /_search{ \"query\": { \"match_phrase\": { \"message\": \"this is a test\" } }} 可以通过slop参数来控制单词之间的允许间隔: 1234567891011GET /_search{ \"query\": { \"match_phrase\": { \"message\": { \"query\": \"this is a test\", \"slop\": 2 } } }} multi match query如果你需要在多个字段上进行文本搜索，可用multi_match 。multi_match在 match的基础上支持对多个字段进行文本查询。 123456789GET ftq/_search{ \"query\": { \"multi_match\" : { \"query\": \"lucene java\", \"fields\": [ \"title\", \"content\" ] } }} 还可以使用*匹配多个字段 123456789GET ftq/_search{ \"query\": { \"multi_match\" : { \"query\": \"lucene java\", \"fields\": [ \"title\", \"cont*\" ] } }} query_string query可以使用query_string查询来创建复杂的搜索，其中包括通配符，跨多个字段的搜索等等。尽管用途广泛，但查询是严格的，如果查询字符串包含任何无效语法，则返回错误。 123456789GET /_search{ \"query\": { \"query_string\": { \"query\": \"(new york city) OR (big apple)\", \"default_field\": \"content\" } }} 搜索多字段 123456789GET /_search{ \"query\": { \"query_string\": { \"fields\": [ \"content\", \"name\" ], \"query\": \"this AND that\" } }} 复合查询语句 (Compound query clauses) 用于合并其他的叶查询或复合查询语句，也就是说复合语句之间可以嵌套，用来表示一个复杂的单一查询，类比mysql的where多条件查询，es的复合查询包括Constant Score Query、Bool Query、Dis Max Query、Function Score Query、Boosting Query，这里详细说一说用的比较多的Bool Query。 Bool QueryBool 查询用bool操作来组合多个查询字句为一个查询。 可用的关键字： 参数 说明 must 根据must中的条件过滤文档，返回的结果文档必须严格匹配条件，会影响相关性算分 filter 根据must中的条件过滤文档，返回的结果文档必须严格匹配条件，和must不同的是，filter不会影响相关性算分 should 根据should中的条件进行筛选，返回的结果文档应该包含should的条件，影响相关性 算分 must_not 根据must_not中的条件过滤文档，返回的结果文档必须不包含must_not条件，会影响相关性算分 123456789101112131415161718192021222324POST _search{ \"query\": { \"bool\" : { \"must\" : { \"term\" : { \"user\" : \"kimchy\" } }, \"filter\": { \"term\" : { \"tag\" : \"tech\" } }, \"must_not\" : { \"range\" : { \"age\" : { \"gte\" : 10, \"lte\" : 20 } } }, \"should\" : [ { \"term\" : { \"tag\" : \"wow\" } }, { \"term\" : { \"tag\" : \"elasticsearch\" } } ], \"minimum_should_match\" : 1, \"boost\" : 1.0 } }} 1、must、must_not、should支持数组，同时filter的查询语句，es会对其进行智能缓存，因此执行效率较高，在不需要算分的查询语句中，可以考虑使用filter替代普通的query语句; 2、查询语句同时包含must和should时，可以不满足should的条件，因为must条件优先级高于should，但是如果也满足should的条件，则会提高相关性算分; 3、可以使用minimum_should_match参数来控制应当满足条件的个数或百分比; 4、must、must_not语句里面如果包含多个条件，则各个条件间是且的关系，而should的多个条件是或的关系。 Filter和Query的异同一个查询语句究竟具有什么样的行为和得到什么结果，主要取决于它到底是处于查询上下文(Query Context) 还是过滤上下文(Filter Context)。 Query context 查询上下文这种语句在执行时既要计算文档是否匹配，还要计算文档相对于其他文档的匹配度有多高，匹配度越高，_score 分数就越高 Filter context 过滤上下文过滤上下文中的语句在执行时只关心文档是否和查询匹配，不会计算匹配度，也就是得分。 下面来看一个例子: 123456789101112131415161718192021222324252627282930313233GET /_search{ \"query\": { //参数表示整个语句是处于 query context 中 \"bool\": { //bool 和 match 语句被用在 query context 中，也就是说它们会计算每个文档的匹配度（_score) \"must\": [ { \"match\": { \"title\": \"Search\" } }, { \"match\": { \"content\": \"Elasticsearch\" } } ], \"filter\": [ //这个子查询处于 filter context 中 { \"term\": { //语句中的 term 和 range 语句用在 filter context 中，它们只起到过滤的作用，并不会计算文档的得分。 \"status\": \"published\" } }, { \"range\": { \"publish_date\": { \"gte\": \"2015-01-01\" } } } ] } }} 结论： 1 查询上下文中，查询操作不仅仅会进行查询，还会计算分值，用于确定相关度；在过滤器上下文中，查询操作仅判断是否满足查询条件 2 过滤器上下文中，查询的结果可以被缓存。 Elasticsearch Url地址说明拿以下添加内容命令举例说明 12345curl -X PUT \"localhost:9200/customer/_doc/1?pretty\" -H 'Content-Type: application/json' -d'{ \"name\": \"John Doe\"}' URL 说明 Localhost Es服务地址 /customer index名，类似Mysql中的库名 /_doc 字段类型 /1 id 索引id pretty 将返回的信息以可读的JSON形式返回。 “name”: “John Doe” 添加内容 常用URL句子 获得节集群中的节点列表 1curl 'xxx:9200/_cat/nodes?v' 检查集群健康 1curl 'xxx:9200/_cat/health?v' 列出所有的索引 1curl 'xxx:9200/_cat/indices?v' 查看ES哪些进程在消耗资源 1curl xxx:9200/_nodes/hot_threads?pretty 查看某条语句的性能分析状况 1curl xxx:9200/xxx/_search?pretty -d '{\"profile\": true, \"query\" : { \"match\" : { \"xxx\" : \"xxx\" } }}' 对某个index进行强制合并segment，提高查询效率 1curl xxx:9200/xxx/_forcemerge?pretty 节点状态查询 1curl xxx:9200/_nodes/stats?pretty 常用URL字段index操作默认之前必须携带index前缀，比如： /customer/_search URL 说明 /_search 搜索 /_aliases 获取或操作索引的别名 /_count 返回符合条件的文档数 /type 创建或操作 类型 /_mapping 创建或操作 mapping /_settings 创建或操作 设置 (number_of_shards是不可更改的) /_open 打开被关闭的索引 /_close 关闭索引 /_refresh 刷新索引（使新加内容对搜索可见） /_flush 刷新索引将变动提交到lucene索引文件中并清空elasticsearch的transaction log，与refresh的区别需要继续研究 /_optimize 优化segement，个人认为主要是对segement进行合并 /_status 获得索引的状态信息 /_segments 获得索引的segments的状态信息 /_explain 不执行实际搜索，而返回解释信息 /_analyze 不执行实际搜索，根据输入的参数进行文本分析 /index/type/id 操作指定文档 /index/type/id/_create 创建一个文档，如果该文件已经存在，则返回失败 /index/type/id/_update 更新一个文件，如果改文件不存在，则返回失败 Nodes操作 URL 说明 /_nodes/process 我主要看file descriptor 这个信息 /_nodes/process/stats 统计信息（内存、CPU能） /_nodes/jvm 获得各节点的虚拟机统计和配置信息 /_nodes/jvm/stats 更详细的虚拟机信息 /_nodes/http 获得各个节点的http信息（如ip地址） /_nodes/http/stats 获得各个节点处理http请求的统计情况 /_nodes/thread_pool 获得各种类型的线程池（elasticsearch分别对不同的操作提供不同的线程池）的配置信息 /_nodes/thread_pool/stats 获得各种类型的线程池的统计信息 以上这些操作和可以通过如下方式 12345/_nodes/${nodeId}/jvm/stats/_nodes/${nodeip}/jvm/stats/_nodes/${nodeattribute}/jvm/stats 针对指定节点的操作。 Distributed操作 URL 说明 /_cluster/nodes 获得集群中的节点列表和信息 /_cluster/health 获得集群信息 /_cluster/state 获得集群里的所有信息（集群信息、节点信息、mapping信息等） Elasticsearch 返回体说明 took – Elasticsearch运行查询所需的时间（以毫秒为单位） timed_out –搜索请求是否超时 _shards –搜索了多少个分片，以及成功，失败或跳过了多少个分片。 max_score –找到的最相关文件的分数 hits.total.value -找到了多少个匹配的文档 hits.sort -文档的排序位置（不按相关性得分排序时） hits._score-文档的相关性得分（使用时不适用match_all） Elasticsearch 基本数据类型 字符串 string 数字类型 long integer double 等 日期 date 布尔类型 boolean 二进制 binary 复杂的数据类型 数组类型 对象类型 嵌套类型 netsted 地理数据类型 专门数据类型 ipv4 完成数据类型 单词计数类型 参考文章: ES的常规指令集合 ES官方文档 ES-聚合-aggs ElasticSearch 系列文章 DSL介绍 Elastic Search之Search API(Query DSL)、字段类查询、复合查询","link":"/2020/08/06/Elasticsearch-%E5%88%9D%E6%AC%A1%E4%BD%BF%E7%94%A8/"},{"title":"Hexo 初次使用记录","text":"安装安装前提，首先需要安装两个软件： Node.js 用于安装Hexo Git 用于代码提交和外网访问 详细的安装方式可自行根据自己的系统百度搜索安装方式，以下是参考网站： https://hexo.io/zh-cn/docs/ 安装 Hexo以上两个软件装完后，即可通过npm安装Hexo 1$ npm install -g hexo-cli 然后执行以下命令指定的文件下会自动创建所需要的文件 123$ hexo init &lt;folder&gt; $ cd &lt;folder&gt;$ npm install 运行完成后会出现如下文字： 1INFO Start blogging with Hexo! 恭喜你，安装完成。 新建完后，指定文件夹下会出现的文件如下 12345678.├── _config.yml├── package.json├── scaffolds├── source| ├── _drafts| └── _posts└── themes 初步使用Hexo本地运行Hexo安装好后，执行以下命令: 12$ hexo g #generate 生成静态文件$ hexo s #server 启动服务器。默认情况下，访问网址为： [http://localhost:4000/](https://link.jianshu.com/?t=http://localhost:4000/) 123INFO Validating configINFO Start processingINFO Hexo is running at http://localhost:4000 . Press Ctrl+C to stop. 出现这段文字即可访问 http://localhost:4000 , 页面如下 至此，本地Hexo搭建完成 常用命令12345$ hexo n \"myblog\" # =&gt; hexo new \"myblog\"$ hexo p # =&gt; hexo publish$ hexo g # =&gt; hexo generate$ hexo s # =&gt; hexo server$ hexo d # =&gt; hexo deploy 服务器命令1234567$ hexo server # Hexo 会监视文件变动并自动更新，无须重启服务器$ hexo server -s # 静态模式$ hexo server -p 5000 # 更改端口$ hexo server -i 192.168.1.1 # 自定义 IP$ hexo clean # 清除缓存，网页正常情况下可以忽略此条命令$ hexo g # 生成静态网页$ hexo d # 开始部署 连接Hexo和GitGit SSH配置关于SSH使用SSH协议，您可以连接到远程服务器和服务并进行身份验证。使用SSH密钥，您可以连接到GitHub，而无需在每次访问时都提供用户名或密码。 检查现有的SSH密钥以下是Mac操作系统： 打开 Terminal. 输入ls -al ~/.ssh以查看是否存在现有的SSH密钥： 12$ ls -al ~/.ssh# Lists the files in your .ssh directory, if they exist 检查目录列表，以查看是否已经有公共SSH密钥。默认情况下，公共密钥的文件名是以下之一： id_rsa.pub id_ecdsa.pub id_ed25519.pub 如果您没有现有的公钥和私钥对，或者不希望使用任何可用于连接到GitHub，可以重新申请 如果您看到列出的现有公共和私有密钥对（例如id_rsa.pub和id_rsa），您希望将它们用于连接到GitHub，则可以将SSH密钥添加到ssh-agent。 生成新的SSH密钥 打开终端。 粘贴以下文本，替换为您的GitHub电子邮件地址。 1$ ssh-keygen -t rsa -b 4096 -C \"your_email@example.com\" 使用提供的电子邮件作为标签，这将创建一个新的ssh密钥。 1&gt; Generating public/private rsa key pair. 当提示您“输入要在其中保存密钥的文件”时，请按Enter。这接受默认文件位置。 1&gt; Enter a file in which to save the key (/Users/you/.ssh/id_rsa): [Press enter] 在提示符下，键入一个安全密码。有关更多信息，请参阅“使用SSH密钥密码短语”。 12&gt; Enter passphrase (empty for no passphrase): [Type a passphrase]&gt; Enter same passphrase again: [Type passphrase again] 将SSH密钥添加到ssh-agent在将新的SSH密钥添加到ssh-agent来管理密钥之前，您应该已经检查了现有的SSH密钥并生成了一个新的SSH密钥。将SSH密钥添加到代理时，请使用默认的macOS ssh-add命令，而不要使用macports，homebrew或其他一些外部源安装的应用程序。 在后台启动ssh-agent。 12$ eval \"$(ssh-agent -s)\"&gt; Agent pid 59566 如果您使用的是macOS Sierra 10.12.2或更高版本，则需要修改~/.ssh/config文件以将密钥自动加载到ssh-agent中并将密码短语存储在密钥链中。 首先，检查~/.ssh/config文件是否存在于默认位置。 12$ open ~/.ssh/config&gt; The file /Users/you/.ssh/config does not exist. 如果文件不存在，请创建文件。 1$ touch ~/.ssh/config 打开~/.ssh/config文件，然后修改文件，~/.ssh/id_rsa如果您没有使用默认位置和id_rsa密钥名称，则将其替换。 1234Host * AddKeysToAgent yes UseKeychain yes IdentityFile ~/.ssh/id_rsa 将您的SSH私钥添加到ssh-agent并将密码短语存储在钥匙串中。如果您使用其他名称创建密钥，或者要添加具有其他名称的现有密钥，请使用私有密钥文件的名称替换命令中的id_rsa。 1$ ssh-add -K ~/.ssh/id_rsa GitHub账户添加新的SSH密钥在将新的SSH密钥添加到GitHub帐户之前，您应该具有： 检查现有的SSH密钥 生成一个新的SSH密钥并将其添加到ssh-agent 将新的SSH密钥添加到GitHub帐户后，您可以重新配置任何本地存储库以使用SSH。有关更多信息，请参阅“将远程URL从HTTPS切换到SSH”。 注意：不再支持DSA密钥（SSH-DSS）。现有密钥将继续起作用，但是您不能将新的DSA密钥添加到GitHub帐户。 将SSH密钥复制到剪贴板。 如果SSH密钥文件的名称与示例代码的名称不同，请修改文件名以匹配当前设置。复制密钥时，请勿添加任何换行符或空格。 12$ pbcopy &lt; ~/.ssh/id_rsa.pub# Copies the contents of the id_rsa.pub file to your clipboard 提示：如果pbcopy不起作用，则可以找到隐藏的.ssh文件夹，在您喜欢的文本编辑器中打开文件，然后将其复制到剪贴板。 在任何页面的右上角，点击您的个人资料照片，然后点击设置。 在用户设置边栏中，点击SSH和GPG密钥。 单击“ 新建SSH密钥”或“ 添加SSH密钥”。 在“标题”字段中，为新密钥添加一个描述性标签。例如，如果您使用的是个人Mac，则可以将此键称为“个人MacBook Air”。 将您的密钥粘贴到“密钥”字段中。 单击添加SSH密钥。 如果出现提示，请确认您的GitHub密码。 测试SSH连接 打开终端。 输入以下内容： 12$ ssh -T git@github.com# Attempts to ssh to GitHub 您可能会看到如下警告： 123&gt; The authenticity of host 'github.com (IP ADDRESS)' can't be established.&gt; RSA key fingerprint is 16:27:ac:a5:76:28:2d:36:63:1b:56:4d:eb:df:a6:48.&gt; Are you sure you want to continue connecting (yes/no)? 或像这样： 123&gt; The authenticity of host 'github.com (IP ADDRESS)' can't be established.&gt; RSA key fingerprint is SHA256:nThbg6kXUpJWGl7E1IGOCspRomTxdCARLviKw6E5SY8.&gt; Are you sure you want to continue connecting (yes/no)? 验证您看到的消息中的指纹与步骤2中的消息之一匹配，然后键入yes： 12&gt; Hi username! You've successfully authenticated, but GitHub does not&gt; provide shell access. 以上Git SSH部分全部配置完成。 Git 仓库配置首先打开github，点击New repository，创建一个新仓库，仓库名必须要遵守格式：账户名.github.io，不然接下来会有很多麻烦。并且需要勾选Initialize this repository with a README。如下图 然后项目就建成了，点击Settings，向下拉到最后有个GitHub Pages，点击Choose a theme选择一个主题。然后等一会儿，再回到GitHub Pages，会变成下面这样： 点击那个链接，就会出现自己的网页啦，效果如下： 以上Git仓库配置完毕！ Hexo 配置在安装Hexo的根目录下找到_config.yml文件，修改Deployment配置 123456# Deployment## Docs: https://hexo.io/docs/deployment.htmldeploy: type: git #连接方式 repository: git@github.com:zzdespair/zzdespair.github.io.git #地址 在下载Code中能看到 branch: master #选择分支 默认master 主分支 Hexo部署到git我们需要安装hexo-deployer-git插件,在bHexo的根目录下运行一下命令进行安装 1$ npm install hexo-deployer-git --save 然后输入hexo new post &quot;article title&quot;，新建一篇文章。 然后打开Hexo\\source\\_posts的目录，可以发现下面多了一个文件夹和一个.md文件，一个用来存放你的图片等数据，另一个就是你的文章文件啦。 编写完markdown文件后，根目录下输入hexo g生成静态网页，然后输入hexo s可以本地预览效果，最后输入hexo d上传到github上。这时打开你的github.io主页就能看到发布的文章啦。 参考文章- [1] Hexo官方文档 - [2] 简书-使用hexo搭建github博客 - [3] 知乎-超详细Hexo+Github博客搭建小白教程 - [4] 简书-Hexo 搭建个人博客 #01 框架的本地安装与运行 - [5] 在本地搭建Hexo博客框架，并部署到Github - [6] Connecting to GitHub with SSH","link":"/2020/08/04/hello-hexo/"},{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2020/08/04/hello-world/"}],"tags":[{"name":"Elasticsearch","slug":"Elasticsearch","link":"/tags/Elasticsearch/"},{"name":"Hexo","slug":"Hexo","link":"/tags/Hexo/"},{"name":"Blog","slug":"Blog","link":"/tags/Blog/"}],"categories":[]}